---
title: "Housing Price vs. Supply 2024"
author: Stephanie Armstrong, Ilgaz Kuscu, Alexander D. Silberman, Baylee Wechsler
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
bibliography: references.bib
csl: https://www.zotero.org/styles/apa
---

```{r clear environment}
rm(list=ls())
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      root.dir = "C:\\Users\\alexa\\Code\\GW DATS\\6101 Intro to Data Science\\Project 2\\GitHub",
                      warning = F, message = F)
options(scientific = T, digits = 3)
options(scipen=9, digits = 3) 

```

```{r libraries}
library(ezids)
library(tidyverse)
library(janitor)
library(scales)
library(ggrepel)
library(corrplot)
library(tigris)
library(sf)
library(ggridges)
library(scales)
library(tidycensus)
library(here)
library(car)
library(caTools)
library(glmnet)
library(randomForest)
library(rpart)
library(rpart.plot)
library(rattle)
library(kableExtra)
library(ModelMetrics)
```

## Introduction

<!-- Supply and demand is a classic and well-known economic model. However, real life is far more nuanced than the simple version of this model that many are familiar with. We wanted to use real housing price and supply samples to explore how well that relationship fits economic theory. Our research question is: How is the supply of housing related to average home prices across U.S. metropolitan areas in the year 2024? -->

<!-- In 2024, the housing market had more sellers than buyers. Normally, the excess supply in a buyer’s market would place downward pressure on prices, but concurrent economic monetary situation stalled that natural phenomenon. These unusual market conditions make it important to examine their wider economic and social impacts. -->

<!-- Accordingly, the relevance of our question extends beyond economics, since housing prices directly affect households, companies, and wealth inequality. Rising home prices lead to wage increases, which in turn raise the cost of goods and services—creating a vicious cycle that fuels inflation. As uncontrolled inflation deteriorates living standards, companies struggle to attract and retain workers due to reduced labor mobility. Consequently, some firms relocate to more affordable states to balance operational costs. -->

<!-- At the same time, while rising housing costs create financial burdens for renters, homeowners benefit from accumulating wealth through year-over-year appreciation and rental income. In fact, the wealth gap between homeowners and renters has reached a historic high, as housing costs have grown faster than incomes [@urban2023]. -->

<!-- Throughout our analysis, we cross-checked our findings with existing research and market reports to better understand these dynamics. In 2024, there were nearly 500000 more sellers than buyers [@redfin2025]. Despite this apparent “buyer’s market,” prices did not decline as economic theory might predict. One possible explanation is that the elasticity of housing supply—the degree to which new construction responds to price changes—has been steadily declining since the Great Recession [@cepr2024]. -->

<!-- Additionally, median home prices have more than doubled over the past decade, rising faster than both wages and inflation [@uschamber2024]. This growing affordability crisis hinders job creation, encourages corporate relocations, and causes states to lose billions of dollars in economic output [@pebbcap2024]. -->

## Conclusion 
<!-- In conducting this project, our team was reminded of our first day of class, going over the data science life cycle. We initially established a question we wanted to answer and identified data that we believed could enlighten us on this data. However, by conducting our EDA, we discovered that the metrics that AEI chose to include do not directly match the question we sought out to answer. -->

<!-- ### Key Takeaways -->
<!-- There is a clear connection between months supply of housing and median sale price. This could be due to high demand pushing sale prices higher or low supply pushing sale prices higher. We will need more data in order to identify which is the stronger driver, if either.  -->

<!-- We also found interesting connections between the percent of houses in a county that are new constructions and the mortgage default rate in a county with our two key variables, months supply and median sale price.  -->

<!-- ### Next Steps -->
<!-- One next step for us might be to pull in additional data to help us add color to the picture we are painting and to also help us better address our original research question. We will need to do additional analysis to better identify the nature of the relationship between these primary and secondary variables.  -->

<!-- We could also take straight from the source and attempt to pull public records data from First American via Data Tree and listings counts from Zillow and Realtor.com, perhaps adjusting for population. This would be a much better representation of supply than Months' Supply. -->




# project 2 ####

## Data 

This project uses the cbsa-level 2024 Realtor.com dataset referenced as the source data from our Project 1 data source: the American Enterprise Institute. We enrich this dataset with 1-year 2024 data from the American Community Survey. The variables are:  

Original Variable Name | New Variable Name | Definition  
  :-: |  :-: | :-- 
month_date_yyyymm | month_date_yyyymm | Month/Year date in Realtor.com dataset
cbsa_code | cbsa_code | Core-Based Statistical Area (CBSA) code
cbsa_title | cbsa_title | CBSA name/title
median_listing_price_per_square_foot | median_listing_price_per_square_foot | Median listing price per square foot
total_listing_count | total_listing_count | Total number of listings in the CBSA/month
pending_ratio | pending_ratio | Share of listings pending sale
median_days_on_market | median_days_on_market | Median number of days listings are on the market
log_median_price_sqft | log_median_price_sqft | Log of median listing price per square foot 
DP05_0001E | total_population | Total population (ACS) 
DP05_0003PE | pct_female | Percent female 
DP05_0037PE | pct_white | Percent White alone 
DP05_0045PE | pct_black | Percent Black alone 
DP05_0053PE | pct_aian | Percent American Indian/Alaska Native 
DP05_0061PE | pct_asian | Percent Asian alone 
DP05_0069PE | pct_nhpi | Percent Native Hawaiian/Pacific Islander 
DP05_0075PE | pct_two_or_more | Percent two or more races 
DP05_0090PE | pct_hispanic | Percent Hispanic or Latino 
DP05_0018E | median_age | Median age 
DP05_0019PE | pct_under_18 | Percent under 18 years old 
DP02_0001E | total_households | Total households 
DP02_0002PE | pct_married_couple | Percent married-couple households 
DP02_0006PE | pct_male_householder | Percent male householder, no spouse 
DP02_0010PE | pct_female_householder | Percent female householder, no spouse 
DP03_0005PE | pct_unemployed | Percent unemployed 
DP03_0119PE | pct_below_poverty | Percent below poverty level 
DP03_0053PE | pct_income_10_14999 | Percent income $10k–$14,999 
DP03_0054PE | pct_income_15_24999 | Percent income $15k–$24,999 
DP03_0055PE | pct_income_25_34999 | Percent income $25k–$34,999 
DP03_0056PE | pct_income_35_49999 | Percent income $35k–$49,999 
DP03_0057PE | pct_income_50_74999 | Percent income $50k–$74,999 
DP03_0058PE | pct_income_75_99999 | Percent income $75k–$99,999 
DP03_0059PE | pct_income_100_149k | Percent income $100k–$149k 
DP03_0060PE | pct_income_150_199k | Percent income $150k–$199k 
DP03_0061PE | pct_income_200k_plus | Percent income $200k+ 
DP03_0062E | median_household_income | Median household income 
DP02_0067PE | pct_high_school_plus | Percent high school or higher 
DP02_0068PE | pct_bachelors_plus | Percent bachelor's degree or higher 
DP04_0001E | total_housing_units | Total housing units 
DP04_0003PE | pct_vacant_units | Percent vacant housing units 
DP04_0046PE | pct_owner_occupied | Percent owner-occupied housing units 
DP04_0017PE | pct_built_2020_plus | Percent built 2020+ 
DP04_0018PE | pct_built_2010_2019 | Percent built 2010–2019 
DP04_0019PE | pct_built_2000_2009 | Percent built 2000–2009 
DP04_0020PE | pct_built_1990_1999 | Percent built 1990–1999 
DP04_0021PE | pct_built_1980_1989 | Percent built 1980–1989 
DP04_0022PE | pct_built_1970_1979 | Percent built 1970–1979 
DP04_0023PE | pct_built_1960_1969 | Percent built 1960–1969 
DP04_0024PE | pct_built_1950_1959 | Percent built 1950–1959 
DP04_0025PE | pct_built_1940_1949 | Percent built 1940–1949 
DP04_0040PE | pct_1_bed | Percent 1-bedroom units 
DP04_0041PE | pct_2_bed | Percent 2-bedroom units 
DP04_0042PE | pct_3_bed | Percent 3-bedroom units 
DP04_0043PE | pct_4_bed | Percent 4-bedroom units
DP04_0044PE | pct_5plus_bed | Percent 5+ bedroom units 

### Data Sources

#### American Community Survey

```{r future ACS variable dictionary}
#ACS variable codes 
#use to make a dictionary for the report

#demographics

#DP05_0001E - Total Population

#DP05_0003PE – % Female
#DP05_0037PE – % White alone
#DP05_0045PE – % Black or African American alone
#DP05_0053PE - % American Indian and Alaska Native alone
#DP05_0061PE – % Asian alone
#DP05_0069PE - % Native Hawaiian and Other Pacific Islander alone
#DP05_0075PE - % Two or More Races

#DP05_0090PE – % Hispanic or Latino

#DP05_0018E - Median age (years)
#DP05_0019PE - % Under 18 years

#households

#DP02_0001E – Total households
#DP02_0002PE – % Married-couple households
#DP02_0006PE – % Male householder, no spouse
#DP02_0010PE – % Female householder, no spouse

#economics

#DP03_0005PE – % Unemployed
#DP03_0119PE – % Below poverty level

#DP03_0052PE - % Household income <$10,000 (ELIMINATE FOR MULTICOLLINEARITY)
#DP03_0053PE - % Household income $10,000 to $14,999
#DP03_0054PE - % Household income $15,000 to $24,999
#DP03_0055PE - % Household income $25,000 to $34,999
#DP03_0056PE - % Household income $35,000 to $49,999
#DP03_0057PE - % Household income $50,000 to $74,999
#DP03_0058PE – % Household income $75,000 to $99,999
#DP03_0059PE – % Household income $100,000 to $149,999
#DP03_0060PE – % Household income $150,000 to $199,999
#DP03_0061PE – % Household income $200,000 or more

#DP03_0062E - Median household income (dollars)

#DP02_0067PE - % High school graduate or higher
#DP02_0068PE – % Bachelor’s degree or higher


#housing

#DP04_0001E – Total housing units
#DP04_0003PE – % Vacant units

#DP04_0046PE – % of occupied that are Owner-occupied

#DP04_0017PE - % Units built 2020 or later
#DP04_0018PE - % Units built 2010 to 2019
#DP04_0019PE – % Units built 2000 to 2009
#DP04_0020PE – % Units built 1990 to 1999
#DP04_0021PE – % Units built 1980 to 1989
#DP04_0022PE – % Units built 1970 to 1979
#DP04_0023PE – % Units built 1960 to 1969
#DP04_0024PE – % Units built 1950 to 1959
#DP04_0025PE – % Units built 1940 to 1949
#DP04_0026PE – % Units built 1939 or earlier (ELIMINATE FOR MULTICOLLINEARITY)

#DP04_0039PE - % 0 Bedrooms (ELIMINATE FOR MULTICOLLINEARITY)
#DP04_0040PE - % 1 Bedrooms
#DP04_0041PE - % 2 Bedrooms
#DP04_0042PE - % 3 Bedrooms
#DP04_0043PE - % 4 Bedrooms
#DP04_0044PE - % 5 Bedrooms or more
```

```{r load_acs}
census_api_key("29e7dfea2f8b253a0a10ccd9626f78e49f4f0a4f")

# census_vars = load_variables(year = 2024, dataset = "acs1/profile", cache = TRUE)
# census_vars_limited = grepv("^(?!DP02PR).*P$", census_vars$name, perl=TRUE)
# acs1_2024 = get_acs(geography = "metropolitan statistical area/micropolitan statistical area",
#                     variables = census_vars_limited,
#                     year = 2024,
#                     survey = "acs1",
#                     output = "wide")
# head(acs1_2024)
```


```{r new_load_acs}
vars_needed <- c("NAME",
                 "GEOID",
                 "DP05_0001E", 
                 "DP05_0003PE", 
                 "DP05_0037PE", 
                 "DP05_0045PE", 
                 "DP05_0053PE", 
                 "DP05_0061PE", 
                 "DP05_0069PE", 
                 "DP05_0075PE", 
                 "DP05_0090PE", 
                 "DP05_0018E", 
                 "DP05_0019PE", 
                 "DP02_0001E", 
                 "DP02_0002PE", 
                 "DP02_0006PE", 
                 "DP02_0010PE", 
                 "DP03_0005PE", 
                 "DP03_0119PE", 
                 # "DP03_0052PE", 
                 "DP03_0053PE", 
                 "DP03_0054PE", 
                 "DP03_0055PE", 
                 "DP03_0056PE", 
                 "DP03_0057PE", 
                 "DP03_0058PE", 
                 "DP03_0059PE",
                 "DP03_0060PE", 
                 "DP03_0061PE", 
                 "DP03_0062E",
                 "DP02_0067PE", 
                 "DP02_0068PE",
                 "DP04_0001E", 
                 "DP04_0003PE", 
                 "DP04_0046PE",
                 "DP04_0017PE", 
                 "DP04_0018PE",
                 "DP04_0019PE", 
                 "DP04_0020PE",
                 "DP04_0021PE", 
                 "DP04_0022PE", 
                 "DP04_0023PE", 
                 "DP04_0024PE",
                 "DP04_0025PE", 
                 # "DP04_0026PE",
                 # "DP04_0039PE", 
                 "DP04_0040PE", 
                 "DP04_0041PE",
                 "DP04_0042PE", 
                 "DP04_0043PE", 
                 "DP04_0044PE")

acs1_2024_orig = get_acs(geography = "metropolitan statistical area/micropolitan statistical area",
                         variables = vars_needed,
                         year = 2024,
                         survey = "acs1",
                         output = "wide")

head(acs1_2024_orig)
```
<!-- The list of variables includes what should be the total population as a percent—"DP05_0001P"—but, as that would always be 100%, the wise people at the US Census Bureau made such equal to "DP05_0001E", the total population estimate, necessitating no further adjustment. -->


```{r dropping margins of error}
acs1_2024_no_M = select(acs1_2024_orig, -grepv("M$", names(acs1_2024_orig)))
head(acs1_2024_no_M)
#This includes both percents as well as margins of error. I (Alex) have included code to drop the margins of error.
```

```{r rename_acs_vars}
#rename ACS variable codes to real things
acs1_2024 <- acs1_2024_no_M %>%
  rename(total_population = DP05_0001E,
    pct_female = DP05_0003PE,
    pct_white = DP05_0037PE,
    pct_black = DP05_0045PE,
    pct_aian = DP05_0053PE,
    pct_asian = DP05_0061PE,
    pct_nhpi = DP05_0069PE,
    pct_two_or_more = DP05_0075PE,
    pct_hispanic = DP05_0090PE,
    median_age = DP05_0018E,
    pct_under_18 = DP05_0019PE,
    total_households = DP02_0001E,
    pct_married_couple = DP02_0002PE,
    pct_male_householder = DP02_0006PE,
    pct_female_householder = DP02_0010PE,
    pct_unemployed = DP03_0005PE,
    pct_below_poverty = DP03_0119PE,
    # pct_income_lt10k = DP03_0052PE,
    pct_income_10_14999 = DP03_0053PE,
    pct_income_15_24999 = DP03_0054PE,
    pct_income_25_34999 = DP03_0055PE,
    pct_income_35_49999 = DP03_0056PE,
    pct_income_50_74999 = DP03_0057PE,
    pct_income_75_99999 = DP03_0058PE,
    pct_income_100_149k = DP03_0059PE,
    pct_income_150_199k = DP03_0060PE,
    pct_income_200k_plus = DP03_0061PE,
    median_household_income = DP03_0062E,
    pct_high_school_plus = DP02_0067PE,
    pct_bachelors_plus = DP02_0068PE,
    total_housing_units = DP04_0001E,
    pct_vacant_units = DP04_0003PE,
    pct_owner_occupied = DP04_0046PE,
    pct_built_2020_plus = DP04_0017PE,
    pct_built_2010_2019 = DP04_0018PE,
    pct_built_2000_2009 = DP04_0019PE,
    pct_built_1990_1999 = DP04_0020PE,
    pct_built_1980_1989 = DP04_0021PE,
    pct_built_1970_1979 = DP04_0022PE,
    pct_built_1960_1969 = DP04_0023PE,
    pct_built_1950_1959 = DP04_0024PE,
    pct_built_1940_1949 = DP04_0025PE,
    # pct_built_1939_earlier = DP04_0026PE,
    # pct_0_bed = DP04_0039PE,
    pct_1_bed = DP04_0040PE,
    pct_2_bed = DP04_0041PE,
    pct_3_bed = DP04_0042PE,
    pct_4_bed = DP04_0043PE,
    pct_5plus_bed = DP04_0044PE)

# #check it out
head(acs1_2024)
```

```{r area type var}
#I want to make a area type var here before merging 
#create area_type var
acs1_2024 <- acs1_2024 %>%
  mutate(area_type = ifelse(grepl("Micro Area",NAME), "Micro", "Metro"))

unique(acs1_2024$area_type)
```

```{r}
#ilgaz' correlation stuff
# acs_clean <- acs1_2024_pcts %>% drop_na()
# num_df <- acs_clean %>% 
#   select(where(is.numeric))
# num_df$dummy_target <- rnorm(nrow(num_df))
#model <- lm(dummy_target ~ ., data = num_df)
#check_collinearity(model)

# head(acs_clean)
# num_df <- acs1_2024_pcts %>% select(where(is.numeric))
# 
# corr_mat <- cor(num_df, use = "pairwise.complete.obs")
# threshold <- 0.80
# 
# high_corr <- which(abs(corr_mat) > threshold,
#                    arr.ind = TRUE)
# 
# # convert to readable table
# high_corr_pairs <- data.frame(
#   var1 = rownames(corr_mat)[high_corr[,1]],
#   var2 = colnames(corr_mat)[high_corr[,2]],
#   corr = corr_mat[high_corr]
# )
# 
# # remove duplicates (since matrix is symmetric)
# high_corr_pairs <- high_corr_pairs[high_corr_pairs$var1 < high_corr_pairs$var2, ]
# 
# 
# high_corr_pairs # 2794 out of 147,696 pairs (around 2%) have an absolute corr >= .80
# 
# #steph's sidequest
# sq_acs <- acs1_2024_pcts
# 
# #numeric-only subset
# sq_acs_num <- sq_acs %>%
#   select(where(is.numeric))
# 
# #ID'ing fake percents and set to na
# sq_acs_num <- sq_acs_num %>%
#   mutate(across(everything(),
#                 ~ ifelse(. > 100, NA, .)))
# 
# #dropping all na columns
# sq_acs_num <- sq_acs_num %>%
#   select(where(~ !all(is.na(.))))
# 
# #converting NA values to median
# sq_acs_num <- sq_acs_num %>%
#   mutate(across(everything(),
#                 ~ ifelse(is.na(.),
#                          median(., na.rm = TRUE),
#                          .)))
# 
# #dropping constant or near-constant cols
# sq_acs_num <- sq_acs_num %>%
#   select(where(function(col) {
#     s <- sd(col, na.rm = TRUE)
#     if (is.na(s)) s <- 0
#     s > 10
#   }))
# 
# #dummy target
# set.seed(123)
# sq_acs_num$fake <- rnorm(nrow(sq_acs_num))
# 
# #fitting dummy linear model
# sq_acs_mod <- lm(fake ~ ., data = sq_acs_num)
# 
# #computing vif
# compute_vif <- function(df){
#   vars <- colnames(df)
#   out <- tibble(variable = vars, vif = NA_real_)
#   
#   for(i in seq_along(vars)){
#     y <- df[[i]]
#     X <- df[, -i, drop = FALSE]
#     
# #fit model for this variable alone
#   mod <- tryCatch(lm(y ~ ., data = as.data.frame(X)),
#                     error = function(e) NULL)
#     
#     if(!is.null(mod)){
#       out$vif[i] <- tryCatch(
#         max(car::vif(mod)),
#         error = function(e) NA_real_
#       )
#     }
#   }
#   
#   out
# }
# 
# vif_results <- compute_vif(sq_acs_num)
# 
# #sort highest VIF at top
# vif_results_sorted <- vif_results %>% arrange(desc(vif))
# 
# vif_results_sorted






#team, should we just delete this chunk?- BW

```



```{r filter acs data down}
#which variables to keep


# #filter down the dataset
# acs1_2024_filtered <- acs1_2024_pcts[, c("GEOID", "NAME", vars_needed)]
# 
# #rename ACS variable codes to real things
# acs1_2024_filtered <- acs1_2024_filtered %>%
#   rename(total_population = DP05_0001E,
#     pct_female = DP05_0003PE,
#     pct_white = DP05_0037PE,
#     pct_black = DP05_0045PE,
#     pct_aian = DP05_0053PE,
#     pct_asian = DP05_0061PE,
#     pct_nhpi = DP05_0069PE,
#     pct_two_or_more = DP05_0075PE,
#     pct_hispanic = DP05_0090PE,
#     median_age = DP05_0018E,
#     pct_under_18 = DP05_0019PE,
#     total_households = DP02_0001E,
#     pct_married_couple = DP02_0002PE,
#     pct_male_householder = DP02_0006PE,
#     pct_female_householder = DP02_0010PE,
#     pct_unemployed = DP03_0005PE,
#     pct_below_poverty = DP03_0119PE,
#     pct_income_lt10k = DP03_0052PE,
#     pct_income_10_14999 = DP03_0053PE,
#     pct_income_15_24999 = DP03_0054PE,
#     pct_income_25_34999 = DP03_0055PE,
#     pct_income_35_49999 = DP03_0056PE,
#     pct_income_50_74999 = DP03_0057PE,
#     pct_income_75_99999 = DP03_0058PE,
#     pct_income_100_149k = DP03_0059PE,
#     pct_income_150_199k = DP03_0060PE,
#     pct_income_200k_plus = DP03_0061PE,
#     median_household_income = DP03_0062E,
#     pct_high_school_plus = DP02_0067PE,
#     pct_bachelors_plus = DP02_0068PE,
#     total_housing_units = DP04_0001E,
#     pct_vacant_units = DP04_0003PE,
#     pct_owner_occupied = DP04_0046PE,
#     pct_built_2020_plus = DP04_0017PE,
#     pct_built_2010_2019 = DP04_0018PE,
#     pct_built_2000_2009 = DP04_0019PE,
#     pct_built_1990_1999 = DP04_0020PE,
#     pct_built_1980_1989 = DP04_0021PE,
#     pct_built_1970_1979 = DP04_0022PE,
#     pct_built_1960_1969 = DP04_0023PE,
#     pct_built_1950_1959 = DP04_0024PE,
#     pct_built_1940_1949 = DP04_0025PE,
#     pct_built_1939_earlier = DP04_0026PE,
#     pct_0_bed = DP04_0039PE,
#     pct_1_bed = DP04_0040PE,
#     pct_2_bed = DP04_0041PE,
#     pct_3_bed = DP04_0042PE,
#     pct_4_bed = DP04_0043PE,
#     pct_5plus_bed = DP04_0044PE)
# 
# # #check it out
# head(acs1_2024_filtered)
# 
# # remove a few collinear / unusable vars
# # acs1_2024_filtered <- acs1_2024_filtered %>%
# #   select(-pct_male,
# #          -pct_hispanic,
# #          -pct_65_over,
# #          -pct_renter_occupied)
# 
# #I want to make a area type var here before merging 
# #create area_type var
# acs1_2024_filtered <- acs1_2024_filtered %>%
#   mutate(area_type = ifelse(grepl("Micro Area",NAME), "Micro", "Metro"))
# 
# unique(acs1_2024_filtered$area_type)


#team, I think we should delete this chunk too. -BW
```


#### Realtor.com

```{r load_realtor}
#loading in inventory data
inventory_raw <- read_csv(here("../RDC_Inventory_Core_Metrics_Metro_History.csv"))

#cleaning names
clean_names(inventory_raw)

#hunting down year
glimpse(inventory_raw)

#filtering for 2024
inventory_2024 <- inventory_raw %>%
  filter(month_date_yyyymm >= 202401 & month_date_yyyymm <= 202412)

#filtered for target vars
inventory_2024_filtered <- inventory_2024%>%
  select(
    month_date_yyyymm,
    cbsa_code,
    cbsa_title,
    median_listing_price_per_square_foot,
    total_listing_count,
    pending_ratio,
    median_days_on_market)%>%
  mutate(
    log_median_price_sqft = log(median_listing_price_per_square_foot)) %>%
  drop_na()

summary(inventory_2024_filtered)
```

```{r grouping}
# inventory_2024_grouped <- inventory_2024_filtered %>%
#   select(!month_date_yyyymm) %>%
#   group_by(cbsa_code, cbsa_title)%>%
#   summarise(
#     med_median_listing_price_per_square_foot = median(median_listing_price_per_square_foot),
#     # avg_log_median_listing_price = mean(log_median_listing_price, na.rm = TRUE),
#     med_total_listing_count = median(total_listing_count),
#     med_pending_ratio = median(pending_ratio),
#     med_median_days_on_market = median(median_days_on_market)) %>%
#   ungroup()
# 
# print(inventory_2024_grouped)
```

The Realtor.com inventory dataset provides monthly metro-level housing indicators like listing counts, median prices, and days on market. Unlike the ACS, it doesn’t include paired estimates or margins of error. Each variable is already given as a direct point estimate. Some fields (the _mm and _yy columns) represent month-over-month or year-over-year percent changes, which introduce redundancy, so we later drop those before modeling. Aside from that, the data only need to be filtered to the 2024 months and are ready for use.


```{r merge datasets & clean}
#cbsa code var to chr so merge works
inventory_2024_filtered <- inventory_2024_filtered %>%
  mutate(cbsa_code = as.character(cbsa_code))

#merge ACS and inventory datasets
full_dataset <- acs1_2024 %>%
  left_join(inventory_2024_filtered, by = c("GEOID" = "cbsa_code"))

#which vars to factor
factor_vars <- c("GEOID",
                 # "NAME",
                 "cbsa_title",
                 # "HouseholdRank",
                 # "quality_flag",
                 "area_type")

#drop redundant columns
redundant_vars <- c("NAME")

#clean dataset
full_dataset_clean <- full_dataset %>% 
  select(-all_of(redundant_vars)) %>% # Drop redundant columns
  mutate(across(all_of(factor_vars), as.factor)) #%>% # Convert factor variables
  # mutate(log_median_listing_price = log(median_listing_price)) # Log home price

#NAs by variable type
na_summary_one <- full_dataset_clean %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "NA_count") %>%
  mutate(var_type = ifelse(variable %in% factor_vars, "factor", "numeric")) %>%
  arrange(desc(NA_count))

print(na_summary_one)

```


```{r NAs}
#address NAs (drop hispanic, when organized by FIPS code, nearest neighbor for ages, replace remaining NAs with 0)
#drop hispanic
#commenting out for now since some of this was done earlier and would like to convene before moving/removing
# full_dataset_clean <- full_dataset_clean %>%
#   select(-pct_hispanic)
# 
# #when organized by FIPS code, nearest neighbor for ages
# age_vars <- c("pct_under_18", "pct_65_over")
# 
# full_dataset_clean <- full_dataset_clean %>%
#   arrange(GEOID, month_date_yyyymm) %>%
#   group_by(GEOID) %>%
#   mutate(across(all_of(age_vars), 
#                 ~ if(sum(!is.na(.)) >= 2) {
#                     approx(x = seq_along(.), 
#                            y = ., 
#                            xout = seq_along(.), 
#                            method = "linear", 
#                            rule = 2)$y
#                   } else {
#                     .  # leave as-is (NA)
#                   })) %>%
#   ungroup()
# 
# na_counts_two <- full_dataset_clean %>%
#   summarise(across(everything(), ~ sum(is.na(.)))) %>%
#   pivot_longer(everything(), names_to = "variable", values_to = "na_count") %>%
#   arrange(desc(na_count))
# 
# na_counts_two
```


```{r NAs 2}
#remove remaining NAs
full_dataset_clean <- full_dataset_clean %>%
  drop_na()
#got rid of 86 observations, but we still have over 6k
nrow(full_dataset_clean)
```

After merging the data to create our holistic dataset, we only dropped 86 rows to remove NA values. This left us with 6,280 observations remaining- more than enough to run a reliable analysis. To prepare for our analysis, we started by creating a summary table of key metrics around some of our key variables. 

```{r summary stats}
summary_long <- full_dataset_clean %>%
  summarise(median_price = median(median_listing_price_per_square_foot, na.rm = TRUE),
    mean_price   = mean(median_listing_price_per_square_foot, na.rm = TRUE),
    sd_price     = sd(median_listing_price_per_square_foot, na.rm = TRUE),
    median_supply = median(total_listing_count, na.rm = TRUE),
    mean_supply   = mean(total_listing_count, na.rm = TRUE),
    sd_supply     = sd(total_listing_count, na.rm = TRUE),
    median_income = median(median_household_income, na.rm = TRUE),
    mean_income   = mean(median_household_income, na.rm = TRUE),
    sd_income     = sd(median_household_income, na.rm = TRUE),
    median_pending_ratio = median(pending_ratio, na.rm = TRUE),
    mean_pending_ratio   = mean(pending_ratio, na.rm = TRUE),
    sd_pending_ratio     = sd(pending_ratio, na.rm = TRUE)) %>%
  pivot_longer(everything(), names_to = "Metric", values_to = "Value")

summary_table_kable <- kable(summary_long, caption = "Summary Statistics") %>%
  kable_styling(full_width = F, bootstrap_options = c("striped", "hover"))

summary_table_kable

```

Even before graphing and investigating our dataset further with exploratory data analysis (EDA), the summary statistics tell us that all of our key variables have a right-skew. For price, income, and pending ratio, this is a slight right-skew, likely as a result of a few, large outliers. However, for our supply variable, there is an extreme right-skew and a very large standard deviation. This is likely due to the structure of the variable instead of a fundamental error in the dataset, and the supply distribution plot in the EDA section will add further context to this variable summary. 

## Exploratory Data Analysis

As this is a new dataset to our team, we needed to start our project with some high-level exploratory data analysis. This data analysis helped us frame our approach to our statistical analysis and predictive models later in the project. For a few of our variables, we graphed both the raw variable distributions and also logged versions of these variables to ensure that we can capture larger relationships independent of influential outliers. 

```{r dist price}
price_dist_plt= ggplot(inventory_2024_filtered, aes(x = median_listing_price_per_square_foot)) +
  geom_histogram(binwidth = 50, fill = "steelblue", color = "black") +
  labs(
    title = "Distribution of Median Listing Price per Square Foot",
    subtitle = "Median Price per Square Foot is heavily right-skewed",
    x = "Median Price per Square Foot",
    y = "Count",
    caption = "SOURCE: Realtor.com") +
  scale_x_continuous(labels= dollar_format(accuracy=1))+
  theme_minimal(base_size = 11)+
  theme(plot.title=element_text(face="bold",size=14,margin=ggplot2::margin(b=3)),
  plot.subtitle = element_text(size=10,color="gray25",margin=ggplot2::margin(b=8)),
  plot.caption  = element_text(size=8,color="gray40",hjust=0),
  axis.title    = element_text(size=10,face="bold"),
  axis.text     = element_text(size=9),
  panel.grid.minor = element_blank(),
  panel.background  = element_blank(),
  plot.background   = element_blank())

ggsave(filename = "price_distribution.png",   
  plot = price_dist_plt,                 
  width = 8,                             
  height = 5,                            
  dpi = 300)

price_dist_plt
```

The first distribution we plotted was median price per square foot- our main dependent variable. The graph shows that a majority of median listing prices per square foot fall below $500 with a handful of very high values pulling the mean of the graph higher than the median. 


```{r dist log price}
logged_price_dist= ggplot(inventory_2024_filtered, aes(x = log_median_price_sqft)) +
  geom_histogram(binwidth = 0.1, fill = "steelblue", color = "black") +
  labs(title = "Distribution of Log Median Listing Price per Square Foot",
    subtitle = "Log transformation reduces right skew and reveals the underlying distribution",
    x = "Log Median Price per Square Foot",
    y = "Count",
    caption = "SOURCE: Realtor.com") +
  scale_x_continuous(
    labels = function(x) dollar(exp(x), accuracy = 1)) +
  theme_minimal(base_size = 11) +
  theme(plot.title    = element_text(face = "bold", size = 14, margin = ggplot2::margin(b = 3)),
    plot.subtitle = element_text(size = 10, color = "gray25", margin = ggplot2::margin(b = 8)),
    plot.caption  = element_text(size = 8, color = "gray40", hjust = 0),
    axis.title    = element_text(size = 10, face = "bold"),
    axis.text     = element_text(size = 9),
    panel.grid.minor = element_blank(),
    panel.background  = element_blank(),
    plot.background   = element_blank())

ggsave(filename = "logged_price_distribution.png",
  plot = logged_price_dist,
  width = 8,
  height = 5,
  dpi = 300)

logged_price_dist
```

Plotting the logged median price per square foot shows a distribution much closer to the normal curve. Logging the price mitigated the heavy influence of of outlier values and creates a more robust source from which to draw relationship insights between variables.


```{r housing count violin}
housing_violion<- ggplot(inventory_2024_filtered, aes(x = "", y = total_listing_count)) +
  geom_violin(fill = "lightblue", color = "steelblue", alpha = 0.7) +
  geom_jitter(width = 0.1, alpha = 0.3, color = "darkblue") +
  labs(title = "Distribution of Total Listing Count",
    subtitle = "Before transformation, the distribution is heavily centered at the lower price range",
    x = "",
    y = "Total Listings",
    caption = "SOURCE: Realtor.com") +
  theme_minimal(base_size = 11) +
  theme(plot.title    = element_text(face="bold", size=14, margin=ggplot2::margin(b=3)),
    plot.subtitle = element_text(size=10, color="gray25", margin=ggplot2::margin(b=8)),
    plot.caption  = element_text(size=8, color="gray40", hjust=0),
    axis.title    = element_text(size=10, face="bold"),
    axis.text     = element_text(size=9),
    panel.grid.minor = element_blank(),
    panel.background  = element_blank(),
    plot.background   = element_blank())

housing_violion
```


This graph attempts to draw further insights into the distribution of listing counts among our cbsa units, but the concentration falling so heavily near the bottom of the y-axis makes it difficult to specifically interpret the supply distribution. To address this, we also plotted the log of supply below. 


```{r log housing count violin}
log_housing_violin_plt= ggplot(inventory_2024_filtered, aes(x = "", y = total_listing_count)) +
  geom_violin(fill = "lightblue", color = "steelblue", alpha = 0.7) +
  geom_jitter(width = 0.1, alpha = 0.3, color = "darkblue") +
  scale_y_log10() +
  labs(title = "Distribution of Total Listing Count (Log Scale)",
    subtitle = "The distribution density of total listing count is more easily visualized\nwith a log transformation and violin plot",
    x = "",
    y = "Log of Total Listings",
    caption = "SOURCE: Realtor.com") +
  theme_minimal(base_size = 11) +
  theme(
    plot.title    = element_text(face="bold", size=14, margin=ggplot2::margin(b=3)),
    plot.subtitle = element_text(size=10, color="gray25", margin=ggplot2::margin(b=8)),
    plot.caption  = element_text(size=8, color="gray40", hjust=0),
    axis.title    = element_text(size=10, face="bold"),
    axis.text     = element_text(size=9),
    panel.grid.minor = element_blank(),
    panel.background  = element_blank(),
    plot.background   = element_blank())

ggsave(filename = "logged_supply_violin.png",   
  plot = log_housing_violin_plt,                 
  width = 8,                             
  height = 5,                            
  dpi = 300)
log_housing_violin_plt
```

This graph shows the log distribution of listing supply overlayed on top of a violin plot of the same data. This added dimension and logged variable adds much more insight into the distribution of the supply variable. As the graph demonstrates, a majority of the supply variables land near the bottom of the y-axis, indicating that there are vastly more cbsa with low listing counts than high listing counts. This is likely due to the structure of our cbsa data. Micro areas can be cbsa regions with 10,000-50,000 residents, and macro areas are any over 50,000 residents. Most small towns fall in the micro level, whereas some massive metropolitan areas (like New York, Chicago, Los Angeles) have millions of residents. This leads to a vast majority of the units falling on the lower end of the spectrum with large, meaninful outliers near the top. For further graphs, we also plotted the log of supply to help normalize this distribution and draw out the true nature of the relationships between variables. 


```{r EDA price vs supply}
price_supply_scatter= ggplot(inventory_2024_filtered, aes(x = total_listing_count, y = median_listing_price_per_square_foot)) +
  geom_point(alpha = 0.6, color = "steelblue") +
  geom_smooth(method = "lm", se = FALSE, color = "darkred", linewidth = 1) +
  labs(title = "Price vs Total Supply",
       subtitle = "Median price per square foot tends to increase as total listings increase",
       x = "Total Listing Count",
       y = "Median Price per Square Foot",
       caption = "SOURCE: Realtor.com") +
  scale_y_continuous(labels = dollar_format(accuracy = 1)) +
  theme_minimal(base_size = 11) +
  theme(plot.title    = element_text(face="bold", size=14, margin=ggplot2::margin(b=3)),
        plot.subtitle = element_text(size=10, color="gray25", margin=ggplot2::margin(b=8)),
        plot.caption  = element_text(size=8, color="gray40", hjust=0),
        axis.title    = element_text(size=10, face="bold"),
        axis.text     = element_text(size=9),
        panel.grid.minor = element_blank(),
        panel.background  = element_blank(),
        plot.background   = element_blank())

ggsave(filename = "pricesupply_scatter.png",   
  plot = price_supply_scatter,                 
  width = 8,                             
  height = 5,                            
  dpi = 300)

price_supply_scatter
```

This graph plots the relationship between the un-transformed price and supply variables as this is the key relationship driving our SMART question. This plot shows how price appears to be positively correlated with total listing count, but we procedded with log transformations of the variables to be sure. 


```{r EDA log price vs supply}
log_price_supply_scatter <- ggplot(inventory_2024_filtered, aes(x = total_listing_count, y = log_median_price_sqft)) +
  geom_point(alpha = 0.6, color = "steelblue") +
  geom_smooth(method = "lm", se = FALSE, color = "darkred", linewidth = 1) +
  labs(title = "Log Price vs Total Supply",
       subtitle = "Log transformation reduces skew and reveals variation across listings",
       x = "Total Listing Count",
       y = "Median Price per Square Foot (log scale)",
       caption = "SOURCE: Realtor.com") +
  scale_y_continuous(labels = function(x) dollar(exp(x), accuracy = 1)) +
  theme_minimal(base_size = 11) +
  theme(plot.title    = element_text(face="bold", size=14, margin=ggplot2::margin(b=3)),
        plot.subtitle = element_text(size=10, color="gray25", margin=ggplot2::margin(b=8)),
        plot.caption  = element_text(size=8, color="gray40", hjust=0),
        axis.title    = element_text(size=10, face="bold"),
        axis.text     = element_text(size=9),
        panel.grid.minor = element_blank(),
        panel.background  = element_blank(),
        plot.background   = element_blank())

log_price_supply_scatter
```


After plotting the lof of our price variable against our supply variable, the positive relationship between price and supply appears even stronger than in our previous graph. However, the distribution of supply is still heavily skewed, so we also logged that variable for the next graph. 

```{r EDA log price vs log supply scatter}
#log by log scatter with a regression line on top and the actual values associated with the log transformation
log_price_log_supply_scatter<- ggplot(inventory_2024_filtered,
       aes(x = log(total_listing_count),
           y = log(median_listing_price_per_square_foot))) +
  geom_point(alpha = 0.6, color = "steelblue") +
  geom_smooth(method = "lm", se = FALSE, color = "darkred", linewidth = 1) +
  scale_x_continuous(name = "Total Listing Count (log scale)",
    labels = function(x) scales::comma(round(exp(x)))) +
  scale_y_continuous(
    name = "Median Price per Square Foot (log scale)",
    labels = function(y) scales::dollar(round(exp(y), 0))) +
  labs(title = "Log Price vs Log Total Supply",
    subtitle = "Higher listing counts are generally associated with higher listing prices",
    caption = "SOURCE: Realtor.com") +
  theme_minimal(base_size = 11) +
  theme(plot.title    = element_text(face="bold", size=14, margin = ggplot2::margin(b=3)),
    plot.subtitle = element_text(size=10, color="gray25", margin = ggplot2::margin(b=8)),
    plot.caption  = element_text(size=8, color="gray40", hjust=0),
    axis.title    = element_text(size=10, face="bold"),
    axis.text     = element_text(size=9),
    panel.grid.minor = element_blank(),
    panel.background  = element_blank(),
    plot.background   = element_blank())

log_price_log_supply_scatter
```

In this final version of our price/ supply scatterplot, we compare the log of price with the log of supply, and we see a clear positive trend between price and supply. Log of median price per square foot steadily increases as log of total listing count increases, supporting the idea that changes in housing supply- at least partially- influence changes in housing price. 


```{r log price by supply boxplot}
logprice_logsupply_scatter= inventory_2024_filtered %>%
  mutate(listing_bin = ntile(total_listing_count, 5)) %>%
  ggplot(aes(x = factor(listing_bin), y = log_median_price_sqft)) +
  geom_boxplot(fill = "lightblue", color = "steelblue") +
  scale_y_continuous(
    name = "Median Price per Square Foot (log scale)",
    labels = function(y) scales::dollar(round(exp(y), 0))) +
  labs(
    title = "Price Distribution by Supply Quantiles",
    subtitle= "Higher median log price is associated with higher supply",
    x = "Supply Quantile (Total Listings)",
    y = "Median Price per Sq Ft (log)",
    caption = "SOURCE: Realtor.com") +
  theme_minimal(base_size = 11) +
  theme(
    plot.title    = element_text(face="bold", size=14, margin = ggplot2::margin(b=3)),
    plot.subtitle = element_text(size=10, color="gray25", margin = ggplot2::margin(b=8)),
    plot.caption  = element_text(size=8, color="gray40", hjust=0),
    axis.title    = element_text(size=10, face="bold"),
    axis.text     = element_text(size=9))

ggsave(filename = "logpricelogsupply_scatter.png",   
  plot = logprice_logsupply_scatter,                 
  width = 8,                             
  height = 5,                            
  dpi = 300)

logprice_logsupply_scatter
```

When separating unlogged supply into quantiles for a boxplot, the positive correlation between these two variables remains plainly visible. 


```{r EDA price vs demand}
Price_demand_scatter= ggplot(inventory_2024_filtered, aes(x = pending_ratio, y = median_listing_price_per_square_foot)) +
  geom_point(alpha = 0.6, color = "steelblue") +
  geom_smooth(method = "lm", se = FALSE, color = "darkred", linewidth = 1) +
  scale_y_continuous(labels = dollar_format(accuracy = 1)) +
  labs(title = "Price vs Market Tightness (Demand)",
    subtitle = "Higher market demand is associated with lower prices",
    x = "Pending Ratio (Pending / Active Listings)",
    y = "Median Price per Square Foot",
    caption = "SOURCE: Realtor.com") +
  theme_minimal(base_size = 11) +
  theme(plot.title    = element_text(face="bold", size=14, margin = ggplot2::margin(b=3)),
    plot.subtitle = element_text(size=10, color="gray25", margin = ggplot2::margin(b=8)),
    plot.caption  = element_text(size=8, color="gray40", hjust=0),
    axis.title    = element_text(size=10, face="bold"),
    axis.text     = element_text(size=9),
    panel.grid.minor = element_blank(),
    panel.background  = element_blank(),
    plot.background   = element_blank())

ggsave(filename = "pricedemand_scatter.png",   
  plot = Price_demand_scatter,                 
  width = 8,                             
  height = 5,                            
  dpi = 300)

Price_demand_scatter
```

The next relationship we investigated was the relationship between housing price and market demand. As is clean in the graph above, plotting median listing price per square foot against pending ratio (our proxy for market demand), reveals a slight negative relationship between price and demand. This is in line with traditional models of supply and demand where where supply is held constant, and higher demand is associated with lower market prices. 


```{r EDA log price vs demand(pending ratio)}
logprice_demand_scatter= ggplot(inventory_2024_filtered, aes(x = pending_ratio, y = log_median_price_sqft)) +
  geom_point(alpha = 0.6, color = "steelblue") +
  geom_smooth(method = "lm", se = FALSE, color = "darkred", linewidth = 1) +
  scale_y_continuous(
    labels = function(x) dollar(exp(x), accuracy = 1)) +
  labs(title = "Log Price vs Market Tightness (Demand)",
    subtitle = "Higher market demand is associated with lower log prices",
    x = "Pending Ratio (Pending / Active Listings)",
    y = "Median Price per Square Foot (log scale)",
    caption = "SOURCE: Realtor.com") +
  theme_minimal(base_size = 11) +
  theme(plot.title    = element_text(face="bold", size=14, margin = ggplot2::margin(b=3)),
    plot.subtitle = element_text(size=10, color="gray25", margin = ggplot2::margin(b=8)),
    plot.caption  = element_text(size=8, color="gray40", hjust=0),
    axis.title    = element_text(size=10, face="bold"),
    axis.text     = element_text(size=9),
    panel.grid.minor = element_blank(),
    panel.background  = element_blank(),
    plot.background   = element_blank())

ggsave(filename = "logpricedemand_scatter.png",   
  plot = logprice_demand_scatter,                 
  width = 8,                             
  height = 5,                            
  dpi = 300)

logprice_demand_scatter
```

We also graphed the relationship between log price and market demand, and the negative correlation between these relationships persists.


```{r EDA price vs median_days_on_market}
price_market_hotness_plot <-ggplot(inventory_2024_filtered, aes(x = median_days_on_market, y = median_listing_price_per_square_foot)) +
  geom_point(alpha = 0.6, color = "steelblue") +
  geom_smooth(method = "lm", se = FALSE, color = "darkred", linewidth = 1) +
  scale_y_continuous(labels = dollar_format(accuracy = 1)) +
  labs(title = "Price vs Market Speed",
    subtitle = "The relationship between price and market speed is minimal",
    x = "Median Days on Market",
    y = "Median Price per Square Foot",
    caption = "SOURCE: Realtor.com") +
  theme_minimal(base_size = 11) +
  theme(plot.title    = element_text(face="bold", size=14, margin = ggplot2::margin(b=3)),
    plot.subtitle = element_text(size=10, color="gray25", margin = ggplot2::margin(b=8)),
    plot.caption  = element_text(size=8, color="gray40", hjust=0),
    axis.title    = element_text(size=10, face="bold"),
    axis.text     = element_text(size=9),
    panel.grid.minor = element_blank(),
    panel.background  = element_blank(),
    plot.background   = element_blank())

price_market_hotness_plot
```

The next set of variables that we investigated in our exploratory data analysis was price and market speed. The graph above shows a slight positive relationship between price and market speed (represented with the median days on market variable), although the magnitude of the relationship appears small. Additionally, we cannot claim causality in this relationship as the risk of reverse causality is very high between these variables. We would require a much more thorough study to investigate any causal relationship between these variables. 

```{r EDA log price vs median_days_on_market}
log_price_market_hotness_plot<- ggplot(inventory_2024_filtered, aes(x = median_days_on_market, y = log_median_price_sqft)) +
  geom_point(alpha = 0.6, color = "steelblue") +
  geom_smooth(method = "lm", se = FALSE, color = "darkred", linewidth = 1) +
  scale_y_continuous(labels = function(x) dollar(exp(x), accuracy = 1)) +
  labs(title = "Log Price vs Market Speed",
    subtitle = "The relationship between log price and market speed is also minimal",
    x = "Median Days on Market",
    y = "Median Price per Square Foot (log scale)",
    caption = "SOURCE: Realtor.com") +
  theme_minimal(base_size = 11) +
  theme(plot.title    = element_text(face="bold", size=14, margin = ggplot2::margin(b=3)),
    plot.subtitle = element_text(size=10, color="gray25", margin = ggplot2::margin(b=8)),
    plot.caption  = element_text(size=8, color="gray40", hjust=0),
    axis.title    = element_text(size=10, face="bold"),
    axis.text     = element_text(size=9),
    panel.grid.minor = element_blank(),
    panel.background  = element_blank(),
    plot.background   = element_blank())

log_price_market_hotness_plot
```
When we graphed the log price against the market speed, the direction of the correlation changed. Instead of having a slightly negative relationship, there now appears to be a very very slight positive relationship between price and market speed. This graph simply adds complexity to the relationship between these two variables- if there is a noteworthy relationship between them at all. 

```{r price vs median household income}
price_income_scatter= ggplot(full_dataset_clean, aes(x = median_household_income, 
                               y = median_listing_price_per_square_foot)) +
  geom_point(alpha = 0.6, color = "steelblue") +
  geom_smooth(method = "lm", se = TRUE, color = "darkred", linewidth = 1) +
  scale_x_continuous(labels = scales::dollar) +
  scale_y_continuous(labels = scales::dollar) +
  labs(title = "Median Price vs Median Household Income",
    subtitle = "There appears to be a strong positive relationship between median\nlisting price and median household income",
    x = "Median Household Income",
    y = "Median Price per Sq Ft",
    caption = "SOURCE: Realtor.com & ACS 2024") +
  theme_minimal(base_size = 11) +
  theme(plot.title    = element_text(face="bold", size=14, margin=ggplot2::margin(b=3)),
    plot.subtitle = element_text(size=10, color="gray25", margin=ggplot2::margin(b=8)),
    plot.caption  = element_text(size=8, color="gray40", hjust=0),
    axis.title    = element_text(size=10, face="bold"),
    axis.text     = element_text(size=9))

ggsave(filename = "priceincome_scatter.png",   
  plot = price_income_scatter,                 
  width = 8,                             
  height = 5,                            
  dpi = 300)

price_income_scatter
```

The final set of variables we analyzed was median price per square foot and median household income. We expected a strong, positive relationship between these variables, and as is plainly visible in the graph above, that is exactly what we found. Again, this is in line with classic models of supply and demand. However, neither of these variables have normal distributions, so we also compared their log transformations. 

```{r log price v log median household income}
logprice_logincome_scatter= ggplot(full_dataset_clean) +
  geom_point(aes(x = log(median_household_income), 
                 y = log(median_listing_price_per_square_foot)),
             alpha = 0.6, color = "steelblue") +
  geom_smooth(aes(x = log(median_household_income), 
                  y = log(median_listing_price_per_square_foot)),
              method = "lm", se = TRUE, color = "darkred", linewidth = 1) +
  scale_x_continuous(name = "Median Household Income (log scale)",
    labels = function(x) scales::dollar(exp(x))) +
  scale_y_continuous(name = "Median Price per Sq Ft (log scale)",
    labels = function(y) scales::dollar(exp(y))) +
  labs(title = "Log Price vs Log Median Household Income",
    subtitle = "Standardizing both variables only emphasizes the relationship",
    caption = "SOURCE: Realtor.com & ACS 2024") +
  theme_minimal(base_size = 11) +
  theme(plot.title    = element_text(face="bold", size=14, margin=ggplot2::margin(b=3)),
    plot.subtitle = element_text(size=10, color="gray25", margin=ggplot2::margin(b=8)),
    plot.caption  = element_text(size=8, color="gray40", hjust=0),
    axis.title    = element_text(size=10, face="bold"),
    axis.text     = element_text(size=9))

ggsave(filename = "logpricelogincome_scatter.png",   
  plot = logprice_logincome_scatter,                 
  width = 8,                             
  height = 5,                            
  dpi = 300)

logprice_logincome_scatter
```
When plotting log price and log income, the positive relationship between the variables is of an even larger magnitude than the un-transformed versions of the variables. This demonstrates a clear and strong relationship and the potential significance of income as a predictor in our models. 

## Modeling

EDA cannot establish causality by itself, but it does provide a strong starting point for more complex statistical testing of the data. 

### Split the Data into train and test

```{r train_test_split}
set.seed(4)

targets = c("median_listing_price_per_square_foot", "log_median_price_sqft")

split = sample.split(full_dataset_clean$GEOID, SplitRatio = 0.8)
train = subset(full_dataset_clean %>% select(-"GEOID"), split == TRUE)
test = subset(full_dataset_clean %>% select(-"GEOID"), split == FALSE)

# train
price_train = train$median_listing_price_per_square_foot
log_price_train = train$log_median_price_sqft

X_train = train %>% select(-all_of(targets))
X_train_matrix = model.matrix(~.-1,X_train)

X_train_no_cbsa = X_train %>% select(-cbsa_title)
X_train_no_cbsa_matrix = model.matrix(~.-1,X_train_no_cbsa)

# test
price_test = test$median_listing_price_per_square_foot
log_price_test = test$log_median_price_sqft

X_test = test %>% select(-all_of(targets))
X_test_matrix = model.matrix(~.-1,X_test)

X_test_no_cbsa = X_test %>% select(-cbsa_title)
X_test_no_cbsa_matrix = model.matrix(~.-1,X_test_no_cbsa)
```


### Linear Regression

#### non-log

```{r}
# names(full_dataset_clean)
#reg d- "avg_log_median_listing_price" i- "total_active_listing_count" 
reg_model_by_1 <- lm(median_listing_price_per_square_foot ~ total_listing_count,
            data = train)

summary(reg_model_by_1)
```


```{r}
#reg d- "avg_log_median_listing_price" i- "total_active_listing_count" "area_type" "avg_pct_income_75k_plus" "avg_pct_units_built_2000_plus" "avg_pct_bachelors_or_higher" 
reg_model_by_all <- lm(median_listing_price_per_square_foot ~ .-cbsa_title-log_median_price_sqft,
            data = train)

summary(reg_model_by_all)
```

Have to eliminate cbsa_title or we would overfit

#### log median listing price
```{r}
# names(full_dataset_clean)
#reg d- "avg_log_median_listing_price" i- "total_active_listing_count" 
reg_model_log_by_1 <- lm(log_median_price_sqft ~ total_listing_count,
            data = train)

summary(reg_model_log_by_1)
```

Nearly double the multiple R-squared.


```{r}
#reg d- "avg_log_median_listing_price" i- "total_active_listing_count" "area_type" "avg_pct_income_75k_plus" "avg_pct_units_built_2000_plus" "avg_pct_bachelors_or_higher" 
reg_model_log_by_all <- lm(log_median_price_sqft ~ . -cbsa_title -median_listing_price_per_square_foot,
            data = train)

summary(reg_model_log_by_all)
```

Have to eliminate cbsa_title or we would overfit

Slightly better fit (also around 2% better, coincidentally)

### LASSO

#### Price
```{r lasso_price}
cv_lasso_price = cv.glmnet(X_train_matrix, price_train, alpha = 1)

lasso_price_best_lambda = cv_lasso_price$lambda.min
lasso_price_best_lambda
```

```{r lasso_price_plot}
plot(cv_lasso_price)
```

```{r lasso_price_best}
lasso_price_best_model = glmnet(X_train_matrix, price_train, alpha = 1, lambda=lasso_price_best_lambda)
coef(lasso_price_best_model)
```

##### Price No CBSA
```{r lasso_price_no_cbsa}
cv_lasso_price_no_cbsa = cv.glmnet(X_train_no_cbsa_matrix, price_train, alpha = 1)

lasso_price_no_cbsa_best_lambda = cv_lasso_price_no_cbsa$lambda.min
lasso_price_no_cbsa_best_lambda
```

```{r lasso_price_no_cbsa_plot}
plot(cv_lasso_price_no_cbsa)
```

```{r lasso_price_no_cbsa_best}
lasso_price_no_cbsa_best_model = glmnet(X_train_no_cbsa_matrix, price_train, alpha = 1, lambda=lasso_price_no_cbsa_best_lambda)
coef(lasso_price_no_cbsa_best_model)
```

#### Log Price
```{r lasso_log_price}
cv_lasso_log_price = cv.glmnet(X_train_matrix, log_price_train, alpha = 1)

lasso_log_price_best_lambda = cv_lasso_log_price$lambda.min
lasso_log_price_best_lambda
```

```{r lasso_log_price_plot}
plot(cv_lasso_log_price)
```

```{r lasso_log_price_best}
lasso_log_price_best_model = glmnet(X_train_matrix, log_price_train, alpha = 1, lambda=lasso_log_price_best_lambda)
coef(lasso_log_price_best_model)
```

##### Log Price No CBSA
```{r lasso_log_price_no_cbsa}
cv_lasso_log_price_no_cbsa = cv.glmnet(X_train_no_cbsa_matrix, log_price_train, alpha = 1)

lasso_log_price_no_cbsa_best_lambda = cv_lasso_log_price$lambda.min
lasso_log_price_no_cbsa_best_lambda
```

```{r lasso_log_price_no_cbsa_plot}
plot(cv_lasso_log_price_no_cbsa)
```

```{r lasso_log_price_no_cbsa_best}
lasso_log_price_no_cbsa_best_model = glmnet(X_train_no_cbsa_matrix, log_price_train, alpha = 1, lambda=lasso_log_price_no_cbsa_best_lambda)
coef(lasso_log_price_no_cbsa_best_model)
```

### Regression Tree


```{r}
## ---- RT_simple ----
options(scipen = 999)   # turn off scientific notation

# # Keep only interpretable economic drivers
# inventory_model_simple <- full_dataset_clean %>%
#   select(
#     median_listing_price_per_square_foot, 
#     total_listing_count,                   
#     pending_ratio,                        
#     median_days_on_market                 
#   )
# 
# # Fit a simple, readable regression tree
# reg_tree_simple <- rpart(
#   median_listing_price_per_square_foot ~
#     total_listing_count + pending_ratio + median_days_on_market,
#   data = inventory_model_simple,
#   method = "anova",
#   control = rpart.control(maxdepth = 3, minbucket = 200)  
# )
# 
# # Plot the tree
# rpart.plot(
#   reg_tree_simple,
#   type = 2,
#   extra = 101,
#   under = TRUE,
#   fallen.leaves = TRUE,
#   roundint = TRUE,
#   main = "Regression Tree: Price per Sq.Ft"
# )

# Basic check: supply distribution
summary(full_dataset_clean$total_listing_count)

#stephs expanded regression tree (median)
tree_data_med <- train %>%
  select(-log_median_price_sqft,
         -cbsa_title) %>%
  na.omit()

set.seed(42)
reg_tree_exp_med <- rpart(median_listing_price_per_square_foot ~ .,
                      data = tree_data_med,
                      method = "anova",
                      control = rpart.control(minbucket = 200))

best_cp_med <- reg_tree_exp_med$cptable[which.min(reg_tree_exp_med$cptable[,"xerror"]), "CP"]
best_cp_med
pruned_tree_data_med <- prune(reg_tree_exp_med, 
                              cp = best_cp_med)

options(repr.plot.width = 28, 
        repr.plot.height = 14)

fancyRpartPlot(reg_tree_exp_med,
               main = "Expanded Regression Tree: Median Price per Sq.Ft",
               sub = paste0("n=", nrow(tree_data_med), "  |  maxdepth=", reg_tree_exp_med$control$maxdepth),
               cex = 1.4,
               tweak = 0.5)   
```


### Random Forest Regression

The plot reports Random Forest variable importance (MSE increase), indicating which features the model relies on most for prediction.
Percent under 18 (pct_under_18) and percent vacant units (pct_vacant_units) are the two most important predictors, suggesting that demographic structure and housing availability prior to a sale include substantial information to predict the price.
The prominence of these variables indicates that the model primarily reflects  sociological factors and pre-sale occupancy conditions, rather than short-term transactional or pricing signals.


```{r}
set.seed(42)
rf_data_med <- train %>%
  mutate(across(where(is.character), as.factor)) %>%
  select(-log_median_price_sqft,-cbsa_title ) %>% #log(price) is basically price. ; cbsa can't be used anyway
  na.omit()                   # Can not handle categorical predictors with more than 53 categories.

rf_exp_med <- randomForest(
  median_listing_price_per_square_foot ~ .,
  data = rf_data_med,
  ntree = 500,
  mtry = floor(sqrt(ncol(rf_data_med) - 1)),
  nodesize = 15,
  importance = TRUE,
  na.action = na.omit
)
imp <- randomForest::importance(rf_exp_med)
imp_df <- data.frame(
  Feature = rownames(imp),
  Importance = imp[, "%IncMSE"]
)

imp_df %>%
  top_n(25, Importance) %>% #top 25 indicators
  ggplot(aes(x=reorder(Feature, Importance), y=Importance)) +
  geom_col(fill="steelblue") +
  coord_flip() +
  labs(title="Random Forest: Top 25 Key Indicators",
       x="Feature",
       y="MSE-contribution (%)") +
  theme_minimal(base_size = 13)
```
#### Random Forest with Log

```{r}
set.seed(42)
rf_data_log <- train %>%
  select(-median_listing_price_per_square_foot,-cbsa_title ) #log(price) is basically price. ; cbsa can't be used anyway                   # Can not handle categorical predictors with more than 53 categories.

rf_exp_log <- randomForest(
  log_median_price_sqft ~ .,
  data = rf_data_log,
  ntree = 500,
  mtry = floor(sqrt(ncol(rf_data_log) - 1)),
  nodesize = 15,
  importance = TRUE,
  na.action = na.omit
)
imp_log <- randomForest::importance(rf_exp_log)
imp_log_df <- data.frame(
  Feature = rownames(imp_log),
  Importance = imp[, "%IncMSE"]
)

imp_log_df %>%
  top_n(25, Importance) %>% #top 25 indicators
  ggplot(aes(x=reorder(Feature, Importance), y=Importance)) +
  geom_col(fill="steelblue") +
  coord_flip() +
  labs(title="Top 25 Key Indicators",
       x="Feature",
       y="MSE-contribution (%)") +
  theme_minimal(base_size = 13)
```


## Model Evaluation

```{r generate predictors}
# models = list(reg_model_by_1, reg_model_by_all, lasso_price_best_model, lasso_price_no_cbsa_best_model, reg_tree_simple, reg_tree_exp_med, rf_exp_med)
# 
# log_models = list(reg_model_log_by_1, reg_model_log_by_all, lasso_log_price_best_model, lasso_log_price_no_cbsa_best_model)

reg_model_by_1_pred = predict(reg_model_by_1, newdata = test)
reg_model_by_all_pred = predict(reg_model_by_all, newdata = test)
lasso_price_best_model_pred = predict(lasso_price_best_model, newx=X_test_matrix)
lasso_price_no_cbsa_best_model_pred = predict(lasso_price_no_cbsa_best_model, newx=X_test_no_cbsa_matrix)
reg_tree_simple_pred = predict(reg_tree_simple, newdata = test)
reg_tree_exp_med_pred = predict(reg_tree_exp_med, newdata = test)
rf_exp_med_pred = predict(rf_exp_med, newdata = test)
reg_model_log_by_1_pred = predict(reg_model_log_by_1, newdata = test)
reg_model_log_by_all_pred = predict(reg_model_log_by_all, newdata = test)
lasso_log_price_best_model_pred = predict(lasso_log_price_best_model, newx=X_test_matrix)
lasso_log_price_no_cbsa_best_model_pred = predict(lasso_log_price_no_cbsa_best_model, newx=X_test_no_cbsa_matrix)
rf_exp_log_pred = predict(rf_exp_log, newdata = test)

reg_model_by_1_rmse = rmse(actual = price_test, predicted = reg_model_by_1_pred)
reg_model_by_all_rmse = rmse(actual = price_test, predicted = reg_model_by_all_pred)
lasso_price_best_model_rmse = rmse(actual = price_test, predicted = lasso_price_best_model_pred)
lasso_price_no_cbsa_best_model_rmse = rmse(actual = price_test, predicted = lasso_price_no_cbsa_best_model_pred)
reg_tree_simple_rmse = rmse(actual = price_test, predicted = reg_tree_simple_pred)
reg_tree_exp_med_rmse = rmse(actual = price_test, predicted = reg_tree_exp_med_pred)
rf_exp_med_rmse = rmse(actual = price_test, predicted = rf_exp_med_pred)
reg_model_log_by_1_rmse = rmse(actual = price_test, predicted = exp(reg_model_log_by_1_pred))
reg_model_log_by_all_rmse = rmse(actual = price_test, predicted = exp(reg_model_log_by_all_pred))
lasso_log_price_best_model_rmse = rmse(actual = price_test, predicted = exp(lasso_log_price_best_model_pred))
lasso_log_price_no_cbsa_best_model_rmse = rmse(actual = price_test, predicted = exp(lasso_log_price_no_cbsa_best_model_pred))
rf_exp_log_rmse = rmse(actual = price_test, predicted = exp(rf_exp_log_pred))


rmse_df = data.frame(Model = c("reg_model_by_1", "reg_model_by_all", "lasso_price_best_model", "lasso_price_no_cbsa_best_model", "reg_tree_simple", "reg_tree_exp_med", "rf_exp_med", "reg_model_log_by_1", "reg_model_log_by_all", "lasso_log_price_best_model", "lasso_log_price_no_cbsa_best_model", "rf_exp_log"),
                     RMSE = c(reg_model_by_1_rmse, reg_model_by_all_rmse, lasso_price_best_model_rmse, lasso_price_no_cbsa_best_model_rmse, reg_tree_simple_rmse, reg_tree_exp_med_rmse, rf_exp_med_rmse, reg_model_log_by_1_rmse, reg_model_log_by_all_rmse, lasso_log_price_best_model_rmse, lasso_log_price_no_cbsa_best_model_rmse, rf_exp_log_rmse)
                     )
  # reg_model_by_1 = reg_model_by_1_rmse,
  #                    reg_model_by_all = reg_model_by_all_rmse,
  #                    lasso_price_best_model = lasso_price_best_model_rmse,
  #                    lasso_price_no_cbsa_best_model = lasso_price_no_cbsa_best_model_rmse,
  #                    reg_tree_simple = reg_tree_simple_rmse,
  #                    reg_tree_exp_med = reg_tree_exp_med_rmse,
  #                    rf_exp_med = rf_exp_med_rmse,
  #                    reg_model_log_by_1 = reg_model_log_by_1_rmse,
  #                    reg_model_log_by_all = reg_model_log_by_all_rmse,
  #                    lasso_log_price_best_model = lasso_log_price_best_model_rmse,
  #                    lasso_log_price_no_cbsa_best_model = lasso_log_price_no_cbsa_best_model_rmse
  #                    )

xkabledply(rmse_df)
```

```{r residual graphs}
residual_graphs <- ggplot(data = data.frame(actual = price_test,
                                            residual = reg_model_by_all_pred - price_test),
                          aes(x = actual, y = residual)) +
  geom_point(alpha = 0.6, color = "steelblue") +
  labs(title = "Residual Plots",
       # subtitle = "Log transformation reduces skew and reveals variation across listings",
       x = "Actual",
       y = "Residual",
       caption = "SOURCE: Realtor.com") +
  # scale_y_continuous(labels = function(x) dollar(exp(x), accuracy = 1)) +
  theme_minimal(base_size = 11) +
  theme(plot.title    = element_text(face="bold", size=14, margin=ggplot2::margin(b=3)),
        plot.subtitle = element_text(size=10, color="gray25", margin=ggplot2::margin(b=8)),
        plot.caption  = element_text(size=8, color="gray40", hjust=0),
        axis.title    = element_text(size=10, face="bold"),
        axis.text     = element_text(size=9),
        panel.grid.minor = element_blank(),
        panel.background  = element_blank(),
        plot.background   = element_blank())

residual_graphs
```


#data manipulation 
EDA- summary statistics(baylee)
Modeling- linear regressions (alex)
Modeling- lasso regression  (alex)
-- probably lots of options here 
Modeling- regression tree (steph)
Modeling- regression forest (ilgaz)



